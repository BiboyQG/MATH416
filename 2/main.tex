\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{array}
\usepackage{fancyhdr}
\usepackage{amssymb}
\usepackage[shortlabels]{enumitem}

\DeclareMathOperator{\R}{\mathbb R}

\pagestyle{fancy}
\fancyhead[L]{Banghao Chi}
\fancyhead[C]{Homework 2}
\fancyhead[R]{13th Feb}

\fancyfoot[C]{\thepage}

\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\footrulewidth}{0.5pt}

\begin{document}

\section*{Exercise 1}
Solve each of the following linear systems by writing down its augmented matrix, doing row operations to get a matrix in reduced row echelon form, and using that to find all of the solutions. You should label your row operations as in section RREF of [Beezer]. \\

(a) 
\begin{align*}
2x_1 + x_2 &= 0 \\
x_1 + x_2 &= 1 \\
3x_1 + 4x_2 &= 5 \\
3x_1 + 5x_2 &= 7
\end{align*}

(b)
\begin{align*}
y_1 + 2y_2 - y_3 &= 1 \\
y_1 + y_2 + 2y_3 &= 0 \\
5y_1 + 8y_2 + y_3 &= 1
\end{align*}

(c)
\begin{align*}
2x_1 + 4x_2 + 5x_3 + 7x_4 &= 18 \\
x_1 + 2x_2 + x_3 - x_4 &= 3 \\
4x_1 + 8x_2 + 7x_3 + 5x_4 &= 24
\end{align*}

\textbf{Solution:} \\

(a) Augmented matrix:
\[
\left[
\begin{array}{cc|c}
2 & 1 & 0 \\
1 & 1 & 1 \\
3 & 4 & 5 \\
3 & 5 & 7
\end{array}
\right].
\]

Perform row operations to get the matrix in reduced row echelon form:

\begin{align*}
& R_1 \leftrightarrow R_2 
\quad \longrightarrow 
\quad
\left[
\begin{array}{cc|c}
1 & 1 & 1 \\
2 & 1 & 0 \\
3 & 4 & 5 \\
3 & 5 & 7
\end{array}
\right] \\
& R_2 \leftarrow R_2 - 2R_1 
\quad \longrightarrow 
\quad
\left[
\begin{array}{cc|c}
1 & 1 & 1\\
0 & -1 & -2\\
3 & 4 & 5\\
3 & 5 & 7
\end{array}
\right] \\
& R_3 \leftarrow R_3 - 3R_1 
\quad \longrightarrow 
\quad
\left[
\begin{array}{cc|c}
1 & 1 & 1 \\
0 & -1 & -2 \\
0 & 1 & 2 \\
3 & 5 & 7
\end{array}
\right] \\
& R_4 \leftarrow R_4 - 3R_1 
\quad \longrightarrow 
\quad
\left[
\begin{array}{cc|c}
1 & 1 & 1 \\
0 & -1 & -2 \\
0 & 1 & 2 \\
0 & 2 & 4
\end{array}
\right] \\
& R_2 \leftarrow -\,R_2 
\quad \longrightarrow 
\quad
\left[
\begin{array}{cc|c}
1 & 1 & 1 \\
0 & 1 & 2 \\
0 & 1 & 2 \\
0 & 2 & 4
\end{array}
\right] \\
& R_1 \leftarrow R_1 - R_2 
\quad R_3 \leftarrow R_3 - R_2 
\quad R_4 \leftarrow R_4 - 2R_2 
\\
&\qquad \longrightarrow 
\quad
\left[
\begin{array}{cc|c}
1 & 0 & -1 \\
0 & 1 & 2 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{array}
\right].
\end{align*}

Hence, we have the system:
\[
\begin{cases}
x_1 = -1 \\
x_2 = 2
\end{cases}
\]

Thus the unique solution is
\[
(x_1, x_2) = (-1,\, 2).
\]

\bigskip

(b) Augmented matrix:
\[
\left[
\begin{array}{ccc|c}
1 & 2 & -1 & 1 \\
1 & 1 & 2 & 0 \\
5 & 8 & 1 & 1
\end{array}
\right]
\]

Perform row operations to get the matrix in reduced row echelon form:

\begin{align*}
& R_2 \leftarrow R_2 - R_1 
\quad \longrightarrow 
\quad
\left[
\begin{array}{ccc|c}
1 & 2 & -1 & 1 \\
0 & -1 & 3 & -1 \\
5 & 8 & 1 & 1
\end{array}
\right] \\
& R_3 \leftarrow R_3 - 5R_1 
\quad \longrightarrow 
\quad
\left[
\begin{array}{ccc|c}
1 & 2 & -1 & 1 \\
0 & -1 & 3 & -1 \\
0 & -2 & 6 & -4
\end{array}
\right] \\
& R_2 \leftarrow -\,R_2 
\quad \longrightarrow 
\quad
\left[
\begin{array}{ccc|c}
1 & 2 & -1 & 1 \\
0 & 1 & -3 & 1 \\
0 & -2 & 6 & -4
\end{array}
\right] \\
& R_3 \leftarrow R_3 + 2R_2 
\quad \longrightarrow 
\quad
\left[
\begin{array}{ccc|c}
1 & 2 & -1 & 1 \\
0 & 1 & -3 & 1 \\
0 & 0 & 0 & -2
\end{array}
\right]
\end{align*}

The last row corresponds to the equation \( 0 = -2 \), which is a contradiction. Therefore, the system is inconsistent and has \textbf{no solutions}.

\bigskip

(c) Augmented matrix:
\[
\left[
\begin{array}{cccc|c}
2 & 4 & 5 & 7 & 18 \\
1 & 2 & 1 & -1 & 3 \\
4 & 8 & 7 & 5 & 24
\end{array}
\right].
\]

Perform row operations to get the matrix in reduced row echelon form:

\begin{align*}
& R_1 \leftrightarrow R_2 
\quad \longrightarrow 
\quad
\left[
\begin{array}{cccc|c}
1 & 2 & 1 & -1 & 3 \\
2 & 4 & 5 & 7 & 18 \\
4 & 8 & 7 & 5 & 24
\end{array}
\right] \\
& R_2 \leftarrow R_2 - 2R_1 
\quad \longrightarrow
\quad
\left[
\begin{array}{cccc|c}
1 & 2 & 1 & -1 & 3 \\
0 & 0 & 3 & 9 & 12 \\
4 & 8 & 7 & 5 & 24
\end{array}
\right] \\
& R_3 \leftarrow R_3 - 4R_1 
\quad \longrightarrow
\quad
\left[
\begin{array}{cccc|c}
1 & 2 & 1 & -1 & 3 \\
0 & 0 & 3 & 9 & 12 \\
0 & 0 & 3 & 9 & 12
\end{array}
\right] \\
& R_3 \leftarrow R_3 - R_2 
\quad \longrightarrow
\quad
\left[
\begin{array}{cccc|c}
1 & 2 & 1 & -1 & 3 \\
0 & 0 & 3 & 9 & 12 \\
0 & 0 & 0 & 0 & 0
\end{array}
\right] \\
& R_2 \leftarrow \frac{1}{3} R_2 
\quad \longrightarrow
\quad
\left[
\begin{array}{cccc|c}
1 & 2 & 1 & -1 & 3 \\
0 & 0 & 1 & 3 & 4 \\
0 & 0 & 0 & 0 & 0
\end{array}
\right] \\
& R_1 \leftarrow R_1 - R_2
\quad \longrightarrow
\quad
\left[
\begin{array}{cccc|c}
1 & 2 & 0 & -4 & -1 \\
0 & 0 & 1 & 3 & 4 \\
0 & 0 & 0 & 0 & 0
\end{array}
\right]
\end{align*}

Hence, we have the system:

\[
\begin{cases}
x_1 + 2x_2 - 4x_4 = -1 \\
x_3 + 3x_4 = 4
\end{cases}
\]

We have 4 variables: \(x_1,x_2,x_3,x_4\) in total, with the pivot variables being \(x_1\) and \(x_3\), while the free variables being \(x_2\) and \(x_4\). \\

Let 
\[
x_2 = s,
\quad
x_4 = t
\]

Then from the second row: \(x_3 = 4 - 3t\).  
From the first row: \(x_1 = -1 - 2s + 4t\).

Hence the general solution is
\[
(x_1, x_2, x_3, x_4)
=
\bigl(-1 - 2s + 4t,\; s,\; 4 - 3t,\; t\bigr), \quad s,t \in \mathbb{R}
\]

\newpage

\section*{Exercise 2}
(a) Suppose $A$ is an $m \times n$ matrix with $m < n$. Consider $A$ as the coefficient matrix of a linear system, where the rightmost column of the augmented matrix is a zero column. Recall from class that we call the solution space of this system, the null space of $A$, which we denote by $\mathcal{N}(A)$. Show that the null space $\mathcal{N}(A)$ contains a nonzero vector by an argument involving the reduced row echelon form of $A$. \\

\noindent
(b) Use part (a) to prove that any $j$ vectors in $\mathbb{R}^k$ are linearly dependent if $j > k$. \\

\textbf{Solution:} \\

(a) Reduced Row Echelon Form (RREF) argument:

\begin{itemize}
\item Write $A$ in its reduced row echelon form (RREF), denoted as $R$.
\[
A \longrightarrow R
\]
\item In the RREF $R$, there are at most $m$ leading columns(since there are only $m$ rows) and therefore at least $n - m$ columns are free columns. Because $m < n$, we see there is at least one free variable in the system
\[
R\,\mathbf{x} = \mathbf{0}.
\]
\item The presence of a free variable guarantees that there is a nontrivial (i.e., not all zeros, and e.g., set all non-pivot variables to 1) solution vector $\mathbf{x}$ to the system $R\,\mathbf{x} = \mathbf{0}$. But $R$ is row-equivalent to $A$, so any solution to $R\,\mathbf{x} = \mathbf{0}$ is also a solution to $A\,\mathbf{x} = \mathbf{0}$.
\item Hence, $\mathcal{N}(A)$ contains a nonzero vector.
\end{itemize}

(b) We use part (a) to prove that any $j$ vectors in $\mathbb{R}^k$ are linearly dependent if $j > k$.

\begin{itemize}
\item Let $\mathbf{v}_1,\mathbf{v}_2,\dots,\mathbf{v}_j \in \mathbb{R}^k$ with $j > k$. 
\item Form an $k \times j$ matrix $A$ by taking these vectors as its columns:
\[
A = 
\begin{pmatrix}
| & | &  & | \\
\mathbf{v}_1 & \mathbf{v}_2 & \cdots & \mathbf{v}_j \\
| & | &  & |
\end{pmatrix}.
\]
\item Note that $A$ has $k$ rows and $j$ columns. Since $j > k$, we have $k < j$, and by part (a), the null space of $A$ must contain a nonzero vector. 
\item Let that nonzero vector be $\mathbf{x} = (x_1, x_2, \dots, x_j)^T \neq \mathbf{0}$ satisfying
\[
A \mathbf{x} = \mathbf{0}.
\]
\item Expanding $A\mathbf{x}$ in terms of the columns $\mathbf{v}_i$ of $A$, we get
\[
A\mathbf{x} = x_1 \mathbf{v}_1 + x_2 \mathbf{v}_2 + \cdots + x_j \mathbf{v}_j = \mathbf{0}.
\]
\item Since $\mathbf{x} \neq \mathbf{0}$, at least one of the $x_i$'s is nonzero, and hence this is a nontrivial linear combination of the $\mathbf{v}_i$ that gives the zero vector.
\item Therefore, $\mathbf{v}_1,\mathbf{v}_2,\dots,\mathbf{v}_j$ are linearly dependent.
\end{itemize}

\newpage

\section*{Exercise 3}
(a) Suppose $S$ is a subset of a vector space $V$. Show that if $v \in V$ is contained in span$(S)$, then span$(S)$ = span$(S \cup \{v\})$. \\

\noindent
(b) Consider $V = \mathbb{R}^2$ and $S = \{(x,y) \mid x \geq 0 \text{ and } y \geq x\}$. Use part (a) to give a short proof that span$(S) = \mathbb{R}^2$ by showing that span$(S)$ contains the vectors $(1,0)$ and $(0,1)$. \\

\textbf{Solution:} \\

(a) Since this is an equal sign, we need to show both $\subseteq$ and $\supseteq$. \\

($\subseteq$): Since $S \subseteq S \cup \{v\}$, we immediately have span$(S) \subseteq$ span$(S \cup \{v\})$. \\

($\supseteq$): Let $w \in$ span$(S \cup \{v\})$. Then $w$ can be written as a linear combination:
\[w = c_1s_1 + c_2s_2 + ... + c_ns_n + kv\]

where $s_i \in S$ and $k \in \mathbb{R}$. Since $v \in$ span$(S)$, we can write:
\[v = a_1s'_1 + a_2s'_2 + ... + a_ms'_m\]

where $s'_i \in S$. Substituting this expression for $v$:
\[w = c_1s_1 + c_2s_2 + ... + c_ns_n + k(a_1s'_1 + a_2s'_2 + ... + a_ms'_m)\]

This shows $w$ is a linear combination of elements in $S$, so $w \in$ span$(S)$. Therefore, span$(S)$ = span$(S \cup \{v\})$. \\

(b) We can see that $(1,1) \in S$ since $1 \geq 0$ and $1 \geq 1$.
Also, $(0,1) \in S$ since $0 \geq 0$ and $1 \geq 0$. \\

Note that $(1,1) - (0,1) = (1,0)$, so $(1,0) \in$ span$(S)$.
We already have $(0,1) \in S \subseteq$ span$(S)$. \\

Therefore, by part (a), since $(1,0), (0,1) \in$ span$(S)$ and these vectors form a basis for $\mathbb{R}^2$, we have:
\[\text{span}(S) = \text{span}(\{(1,0), (0,1)\}) = \mathbb{R}^2\]

\newpage

\section*{Exercise 4}
Let $\mathbf{u}$ and $\mathbf{v}$ be distinct vectors in a vector space $V$. Show that $\{\mathbf{u}, \mathbf{v}\}$ is linearly dependent if and only if one of $\mathbf{u}$ or $\mathbf{v}$ is a scalar multiple of the other. \\

\textbf{Solution:} \\

Since this is an if and only if statement, we need to show both directions. \\

($\Rightarrow$) If $\{\mathbf{u}, \mathbf{v}\}$ is linearly dependent, then there exist scalars $c_1$ and $c_2$, not both zero, such that:
\[c_1\mathbf{u} + c_2\mathbf{v} = \mathbf{0}\]

Since $\mathbf{u}$ and $\mathbf{v}$ are distinct vectors, at least one of them is non-zero. Without loss of generality, assume $\mathbf{u} \neq \mathbf{0}$. \\

If $c_1 = 0$, then $c_2\mathbf{v} = \mathbf{0}$. If $\mathbf{v} = \mathbf{0}$, then this equation is satisfied for any $c_2$, including $c_2 = 0$. If $\mathbf{v} \neq \mathbf{0}$, then we must have $c_2 = 0$. In either case, we would have both $c_1 = 0$ and $c_2 = 0$, contradicting that not both coefficients can be zero in a linear dependence relation. \\

Therefore, $c_1 \neq 0$, and we can rearrange the equation:
\[\mathbf{u} = -\frac{c_2}{c_1}\mathbf{v}\]

Thus, $\mathbf{u}$ is a scalar multiple of $\mathbf{v}$. \\

($\Leftarrow$) Suppose $\mathbf{v} = k\mathbf{u}$ for some scalar $k$. Then:
\[\mathbf{v} - k\mathbf{u} = \mathbf{0}\]

which can be rewritten as:
\[1\mathbf{v} + (-k)\mathbf{u} = \mathbf{0}\]

Since $k$ exists (by assumption) and the coefficients 1 and $-k$ are not both zero, we have shown that $\{\mathbf{u}, \mathbf{v}\}$ is linearly dependent. \\

Therefore, $\{\mathbf{u}, \mathbf{v}\}$ is linearly dependent if and only if one of $\mathbf{u}$ or $\mathbf{v}$ is a scalar multiple of the other.

\newpage

\section*{Exercise 5}
Either prove or give a counterexample to the following statement: If $\mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3$ are three vectors in $\mathbb{R}^3$ none of which is a scalar multiple of another, then they are linearly independent. \\

\textbf{Solution:}
\newpage

\section*{Exercise 6}
In the vector space $\mathcal{F}(\mathbb{R},\mathbb{R})$, all functions from $\mathbb{R}$ to $\mathbb{R}$, consider the elements $f(t) = \sin(t)$ and $g(t) = \cos(t)$. Is the subset $\{f,g\}$ linearly dependent or linearly independent? Prove your answer. \\

\textbf{Solution:}

\end{document}